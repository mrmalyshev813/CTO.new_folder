import logging
import json
import base64
import re
from typing import Dict, List, Optional, Tuple
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from openai import OpenAI
from ..config import settings

logger = logging.getLogger(__name__)


class CompleteWebsiteParser:
    """
    Complete parser workflow that implements all steps from the ticket:
    1. Screenshot with Playwright
    2. Vision analysis with OpenAI
    3. Email scraping
    4. Company owner research
    5. Personalized proposal generation
    6. Auto language detection
    """
    
    def __init__(self):
        self.openai_client = OpenAI(api_key=settings.OPENAI_API_KEY) if settings.OPENAI_API_KEY else None
    
    async def capture_screenshot(self, url: str) -> Tuple[Optional[str], bool, Optional[str]]:
        """
        Capture website screenshot using Playwright.
        
        Returns:
            Tuple of (base64_screenshot, success, error_message)
        """
        logger.info(f'ðŸ“¸ Capturing screenshot for: {url}')
        
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=True,
                    args=['--no-sandbox', '--disable-setuid-sandbox']
                )
                
                page = await browser.new_page(
                    viewport={'width': 1920, 'height': 1080}
                )
                
                # Navigate with timeout
                await page.goto(url, {
                    'wait_until': 'networkidle',
                    'timeout': 30000
                })
                
                # Take screenshot
                screenshot = await page.screenshot(
                    full_page=True,
                    type='png'
                )
                
                logger.info('âœ… Screenshot captured')
                
                # Convert to base64 for OpenAI
                base64_screenshot = base64.b64encode(screenshot).decode('utf-8')
                screenshot_data_url = f'data:image/png;base64,{base64_screenshot}'
                
                await browser.close()
                
                return screenshot_data_url, True, None
                
        except Exception as error:
            logger.error(f'âŒ Screenshot error: {error}')
            return None, False, f'ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ ÑÐºÑ€Ð¸Ð½ÑˆÐ¾Ñ‚: {str(error)}'
    
    async def analyze_screenshot_for_ads(self, url: str, screenshot_data_url: str) -> Tuple[Optional[Dict], bool, Optional[str]]:
        """
        Analyze screenshot using OpenAI Vision API to identify ad placement opportunities.
        
        Returns:
            Tuple of (analysis_result, success, error_message)
        """
        logger.info('ðŸ¤– Analyzing screenshot with OpenAI Vision...')
        
        if not self.openai_client:
            return None, False, 'OpenAI API key is not configured'
        
        try:
            # Remove data URL prefix for API call
            base64_image = screenshot_data_url.split(',')[1]
            
            response = self.openai_client.chat.completions.create(
                model='gpt-4o',  # Supports vision
                messages=[{
                    'role': 'user',
                    'content': [
                        {
                            'type': 'text',
                            'text': f'''ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ ÑÐºÑ€Ð¸Ð½ÑˆÐ¾Ñ‚ ÑÐ°Ð¹Ñ‚Ð° {url} Ð¸ Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸ Ñ€ÐµÐºÐ»Ð°Ð¼Ð½Ñ‹Ðµ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸.

Ð’Ð¸Ð·ÑƒÐ°Ð»ÑŒÐ½Ð¾ Ð¾Ñ†ÐµÐ½Ð¸ Ð³Ð´Ðµ Ð¼Ð¾Ð¶Ð½Ð¾ Ñ€Ð°Ð·Ð¼ÐµÑÑ‚Ð¸Ñ‚ÑŒ Ñ€ÐµÐºÐ»Ð°Ð¼Ñƒ:
1. Header (ÑˆÐ°Ð¿ÐºÐ° ÑÐ°Ð¹Ñ‚Ð°, Ð½Ð°Ð²Ð¸Ð³Ð°Ñ†Ð¸Ñ)
2. Sidebar (Ð±Ð¾ÐºÐ¾Ð²Ð°Ñ Ð¿Ð°Ð½ÐµÐ»ÑŒ ÑÐ¿Ñ€Ð°Ð²Ð° Ð¸Ð»Ð¸ ÑÐ»ÐµÐ²Ð°)  
3. Content (Ð²Ð½ÑƒÑ‚Ñ€Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚Ð°, Ð¼ÐµÐ¶Ð´Ñƒ Ð±Ð»Ð¾ÐºÐ°Ð¼Ð¸)
4. Footer (Ð¿Ð¾Ð´Ð²Ð°Ð» ÑÐ°Ð¹Ñ‚Ð°)
5. Popup (Ð¼Ð¾Ð´Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¾ÐºÐ½Ð°)

Ð”Ð»Ñ ÐºÐ°Ð¶Ð´Ð¾Ð¹ Ð·Ð¾Ð½Ñ‹ ÑƒÐºÐ°Ð¶Ð¸:
- name: Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð·Ð¾Ð½Ñ‹
- available: true ÐµÑÐ»Ð¸ Ð¼ÐµÑÑ‚Ð¾ ÑÐ²Ð¾Ð±Ð¾Ð´Ð½Ð¾, false ÐµÑÐ»Ð¸ ÑƒÐ¶Ðµ Ð·Ð°Ð½ÑÑ‚Ð¾ Ñ€ÐµÐºÐ»Ð°Ð¼Ð¾Ð¹
- size: Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÐ¼Ñ‹Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ€ Ð±Ð°Ð½Ð½ÐµÑ€Ð° (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€ "728x90", "300x250")
- priority: "high" Ð´Ð»Ñ ÑÐ°Ð¼Ñ‹Ñ… Ð·Ð°Ð¼ÐµÑ‚Ð½Ñ‹Ñ… Ð¼ÐµÑÑ‚, "medium" Ð´Ð»Ñ Ð¼ÐµÐ½ÐµÐµ Ð·Ð°Ð¼ÐµÑ‚Ð½Ñ‹Ñ…
- description: Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½Ð¾Ðµ Ð¾Ð¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð³Ð´Ðµ Ð¸Ð¼ÐµÐ½Ð½Ð¾ Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑÑ Ð·Ð¾Ð½Ð° Ð¸ Ð¿Ð¾Ñ‡ÐµÐ¼Ñƒ Ð¾Ð½Ð° Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ð¸Ñ‚

Ð’ÐÐ–ÐÐž: Ð ÐµÐ°Ð»ÑŒÐ½Ð¾ Ð¾Ñ†ÐµÐ½Ð¸Ð²Ð°Ð¹ - ÐµÑÑ‚ÑŒ Ð»Ð¸ ÑÐ²Ð¾Ð±Ð¾Ð´Ð½Ð¾Ðµ Ð¼ÐµÑÑ‚Ð¾ Ð¸Ð»Ð¸ Ð²ÑÑ‘ ÑƒÐ¶Ðµ Ð·Ð°Ð½ÑÑ‚Ð¾.

Ð’ÐµÑ€Ð½Ð¸ JSON:
{{
  "zones": [
    {{
      "name": "Header",
      "available": true,
      "size": "728x90",
      "priority": "high",
      "description": "..."
    }}
  ],
  "language": "ru" or "en" (Ð¾Ð¿Ñ€ÐµÐ´ÐµÐ»Ð¸ ÑÐ·Ñ‹Ðº ÑÐ°Ð¹Ñ‚Ð°)
}}'''
                        },
                        {
                            'type': 'image_url',
                            'image_url': {
                                'url': screenshot_data_url,
                                'detail': 'high'
                            }
                        }
                    ]
                }],
                response_format={'type': 'json_object'},
                max_tokens=2000
            )
            
            result = json.loads(response.choices[0].message.content)
            logger.info('âœ… Vision analysis complete')
            return result, True, None
            
        except Exception as error:
            logger.error(f'âŒ Vision analysis error: {error}')
            return None, False, f'OpenAI Vision API error: {str(error)}'
    
    async def scrape_website_data(self, url: str) -> Dict:
        """
        Scrape website for emails and company information.
        
        Returns:
            Dict with emails, company_name, title, description
        """
        logger.info('ðŸ” Scraping website data...')
        
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=True,
                    args=['--no-sandbox', '--disable-setuid-sandbox']
                )
                
                page = await browser.new_page()
                await page.goto(url, {
                    'wait_until': 'networkidle',
                    'timeout': 30000
                })
                
                html_content = await page.content()
                await browser.close()
                
                soup = BeautifulSoup(html_content, 'html.parser')
                
                # Extract emails
                emails = []
                email_regex = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
                
                # Search in text content
                text = soup.get_text()
                found_emails = re.findall(email_regex, text)
                emails.extend(found_emails)
                
                # Search in mailto links
                for link in soup.find_all('a', href=lambda x: x and x.startswith('mailto:')):
                    email = link['href'].replace('mailto:', '')
                    emails.append(email)
                
                # Extract company name
                company_name = None
                
                # Try meta tags
                company_name = (
                    soup.find('meta', property='og:site_name') or
                    soup.find('meta', attrs={'name': 'author'})
                )
                if company_name:
                    company_name = company_name.get('content')
                else:
                    # Try title
                    title_tag = soup.find('title')
                    if title_tag:
                        company_name = title_tag.get_text().split('|')[0].strip()
                
                # Try footer for Russian company formats
                if not company_name:
                    footer_text = soup.find('footer')
                    if footer_text:
                        footer_content = footer_text.get_text()
                        match = re.search(r'(ÐžÐžÐž|Ð˜ÐŸ|ÐÐž|Ð—ÐÐž|ÐŸÐÐž)\s+["Â«]?([^"Â»\n]+)["Â»]?', footer_content)
                        if match:
                            company_name = match.group(0)
                
                # Clean and deduplicate emails
                unique_emails = list(set(email.strip() for email in emails if email and '@' in email))
                
                result = {
                    'emails': unique_emails,
                    'company_name': company_name,
                    'title': soup.find('title').get_text() if soup.find('title') else None,
                    'description': soup.find('meta', attrs={'name': 'description'}).get('content') if soup.find('meta', attrs={'name': 'description'}) else None
                }
                
                logger.info(f'âœ… Found {len(unique_emails)} emails, company: {company_name}')
                return result
                
        except Exception as error:
            logger.error(f'âŒ Scraping error: {error}')
            return {'emails': [], 'company_name': None, 'title': None, 'description': None}
    
    async def research_company_owner(self, company_name: str, website_url: str) -> Dict:
        """
        Research company information using OpenAI.
        
        Returns:
            Dict with insights about the company
        """
        logger.info('ðŸ”Ž Researching company owner...')
        
        if not company_name:
            return {'insights': 'Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¾ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¸ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°'}
        
        if not self.openai_client:
            return {'insights': 'OpenAI API key is not configured'}
        
        try:
            prompt = f'''ÐÐ°Ð¹Ð´Ð¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¸ "{company_name}" (ÑÐ°Ð¹Ñ‚: {website_url}).

Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ð¾Ð±Ñ‰ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ, Ð½Ð°Ð¹Ð´Ð¸:
1. ÐŸÐ¾Ð»Ð½Ð¾Ðµ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¸ Ð¸ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ñ„Ð¾Ñ€Ð¼Ð° (ÐžÐžÐž, Ð˜ÐŸ Ð¸ Ñ‚.Ð´.)
2. Ð˜Ð¼Ñ Ñ€ÑƒÐºÐ¾Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»Ñ/Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð° (ÐµÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾)
3. ÐžÑÐ½Ð¾Ð²Ð½Ð°Ñ Ð´ÐµÑÑ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ð¸
4. Ð˜Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ñ‹Ðµ Ñ„Ð°ÐºÑ‚Ñ‹ Ð¸Ð»Ð¸ Ð´Ð¾ÑÑ‚Ð¸Ð¶ÐµÐ½Ð¸Ñ

Ð•ÑÐ»Ð¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð½ÐµÑ‚ - Ñ‡ÐµÑÑ‚Ð½Ð¾ Ð½Ð°Ð¿Ð¸ÑˆÐ¸ Ñ‡Ñ‚Ð¾ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾.

Ð’ÐµÑ€Ð½Ð¸ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ð¹ Ð¾Ñ‚Ñ‡Ñ‘Ñ‚ (3-5 Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹) Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ.'''
            
            response = self.openai_client.chat.completions.create(
                model='gpt-4o-mini',
                messages=[{'role': 'user', 'content': prompt}],
                max_tokens=500
            )
            
            insights = response.choices[0].message.content
            logger.info('âœ… Research complete')
            return {'insights': insights}
            
        except Exception as error:
            logger.error(f'âŒ Research error: {error}')
            return {'insights': f'ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¿Ð¾Ð¸ÑÐºÐµ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸: {str(error)}'}
    
    async def generate_personalized_proposal(self, data: Dict) -> str:
        """
        Generate personalized commercial proposal.
        
        Args:
            data: Dict containing website_url, zones, language, company_name, owner_info, emails
            
        Returns:
            Generated proposal text
        """
        logger.info('âœï¸ Generating personalized proposal...')
        
        if not self.openai_client:
            return 'OpenAI API key is not configured. Please set OPENAI_API_KEY environment variable.'
        
        try:
            website_url = data['website_url']
            zones = data['zones']
            language = data['language']
            company_name = data.get('company_name')
            owner_info = data.get('owner_info', {})
            emails = data.get('emails', [])
            
            # Determine template language
            is_english = language == 'en'
            
            available_zones = [z for z in zones if z.get('available')]
            zones_description = '\n'.join([
                f"{i+1}. {zone['name']} â€” {zone['description']}"
                for i, zone in enumerate(available_zones)
            ])
            
            if is_english:
                prompt = f'''Generate a personalized commercial proposal in ENGLISH for advertising placement.
                
Website: {website_url}
Company: {company_name or 'Website owner'}
Owner info: {owner_info.get('insights', 'Not available')}
Available ad zones: 
{zones_description}

Write a professional email following this structure:
1. Greeting (personalized if owner name available)
2. Compliment about their website/content
3. Brief about Adlook company
4. List of advertising opportunities
5. Call to action

Be professional and persuasive. Full email in English.'''
            else:
                prompt = f'''Ð¡Ð³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐ¹ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ ÐºÐ¾Ð¼Ð¼ÐµÑ€Ñ‡ÐµÑÐºÐ¾Ðµ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ðµ Ð½Ð° Ð Ð£Ð¡Ð¡ÐšÐžÐœ ÑÐ·Ñ‹ÐºÐµ.

Ð¡Ð°Ð¹Ñ‚: {website_url}
ÐšÐ¾Ð¼Ð¿Ð°Ð½Ð¸Ñ: {company_name or 'Ð’Ð»Ð°Ð´ÐµÐ»ÐµÑ† ÑÐ°Ð¹Ñ‚Ð°'}
Ð˜Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¾ Ð²Ð»Ð°Ð´ÐµÐ»ÑŒÑ†Ðµ: {owner_info.get('insights', 'ÐÐµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°')}
Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ Ñ€ÐµÐºÐ»Ð°Ð¼Ð½Ñ‹Ðµ Ð¼ÐµÑÑ‚Ð°:
{zones_description}

ÐÐ°Ð¿Ð¸ÑˆÐ¸ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ðµ Ð¿Ð¸ÑÑŒÐ¼Ð¾ Ð¿Ð¾ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ðµ:
1. ÐŸÑ€Ð¸Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ðµ (Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ Ð¸Ð¼Ñ)
2. ÐšÐ¾Ð¼Ð¿Ð»Ð¸Ð¼ÐµÐ½Ñ‚ Ð¿Ñ€Ð¾ Ð¸Ñ… ÑÐ°Ð¹Ñ‚/ÐºÐ¾Ð½Ñ‚ÐµÐ½Ñ‚ (ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¹, Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð½Ð° Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸)
3. ÐšÑ€Ð°Ñ‚ÐºÐ¾ Ð¿Ñ€Ð¾ ÐºÐ¾Ð¼Ð¿Ð°Ð½Ð¸ÑŽ Adlook
4. Ð¡Ð¿Ð¸ÑÐ¾Ðº Ñ€ÐµÐºÐ»Ð°Ð¼Ð½Ñ‹Ñ… Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ÑÑ‚ÐµÐ¹
5. ÐŸÑ€Ð¸Ð·Ñ‹Ð² Ðº Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸ÑŽ

Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÑˆÐ°Ð±Ð»Ð¾Ð½ Ð¸Ð· Ð¿Ñ€Ð¸Ð¼ÐµÑ€Ð° Adlook. Ð‘ÐµÐ· Ð·Ð²Ñ‘Ð·Ð´Ð¾Ñ‡ÐµÐº (*). ÐŸÑ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ‚Ð¾Ð½.'''
            
            response = self.openai_client.chat.completions.create(
                model='gpt-4o-mini',
                messages=[{'role': 'user', 'content': prompt}],
                max_tokens=1500,
                temperature=0.7
            )
            
            proposal = response.choices[0].message.content
            logger.info('âœ… Proposal generated')
            return proposal
            
        except Exception as error:
            logger.error(f'âŒ Proposal generation error: {error}')
            return f'ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ñ: {str(error)}'
    
    async def analyze_website_complete(self, url: str) -> Dict:
        """
        Complete workflow that orchestrates all analysis steps.
        
        Returns:
            Dict with all analysis results
        """
        logger.info('\nðŸš€ === STARTING COMPLETE ANALYSIS ===\n')
        
        try:
            # Step 1: Capture screenshot
            logger.info('STEP 1: Screenshot')
            screenshot_data_url, screenshot_success, screenshot_error = await self.capture_screenshot(url)
            
            if not screenshot_success:
                return {
                    'success': False,
                    'error': f'Failed to capture screenshot: {screenshot_error}'
                }
            
            # Step 2: Vision analysis
            logger.info('\nSTEP 2: Vision Analysis')
            vision_result, vision_success, vision_error = await self.analyze_screenshot_for_ads(url, screenshot_data_url)
            
            if not vision_success:
                return {
                    'success': False,
                    'error': f'Failed to analyze screenshot: {vision_error}'
                }
            
            # Step 3: Scrape website data
            logger.info('\nSTEP 3: Scraping')
            scraped_data = await self.scrape_website_data(url)
            
            # Step 4: Research company
            logger.info('\nSTEP 4: Research')
            owner_info = await self.research_company_owner(scraped_data.get('company_name'), url)
            
            # Step 5: Generate proposal
            logger.info('\nSTEP 5: Generate Proposal')
            proposal = await self.generate_personalized_proposal({
                'website_url': url,
                'zones': vision_result.get('zones', []),
                'language': vision_result.get('language', 'en'),
                'company_name': scraped_data.get('company_name'),
                'owner_info': owner_info,
                'emails': scraped_data.get('emails', [])
            })
            
            logger.info('\nâœ… === ANALYSIS COMPLETE ===\n')
            
            return {
                'success': True,
                'screenshot': screenshot_data_url,
                'zones': vision_result.get('zones', []),
                'language': vision_result.get('language', 'en'),
                'emails': scraped_data.get('emails', []),
                'company_name': scraped_data.get('company_name'),
                'title': scraped_data.get('title'),
                'description': scraped_data.get('description'),
                'owner_info': owner_info.get('insights'),
                'proposal': proposal
            }
            
        except Exception as error:
            logger.error(f'\nâŒ === ANALYSIS FAILED ===')
            logger.error(f'Error: {error}')
            return {
                'success': False,
                'error': str(error)
            }


# Global parser instance
parser = CompleteWebsiteParser()


async def analyze_website_complete(url: str) -> Dict:
    """
    Convenience function to analyze a website completely.
    
    Args:
        url: Website URL to analyze
        
    Returns:
        Dict with complete analysis results
    """
    return await parser.analyze_website_complete(url)