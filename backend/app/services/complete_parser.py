import logging
import json
import base64
import re
from typing import Dict, List, Optional, Tuple
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from openai import OpenAI
from ..config import settings

logger = logging.getLogger(__name__)


class CompleteWebsiteParser:
    """
    Complete parser workflow that implements all steps from the ticket:
    1. Screenshot with Playwright
    2. Vision analysis with OpenAI
    3. Email scraping
    4. Company owner research
    5. Personalized proposal generation
    6. Auto language detection
    """
    
    def __init__(self):
        self.openai_client = OpenAI(api_key=settings.OPENAI_API_KEY) if settings.OPENAI_API_KEY else None
    
    async def capture_screenshot(self, url: str) -> Tuple[Optional[str], bool, Optional[str]]:
        """
        Capture website screenshot using Playwright.
        
        Returns:
            Tuple of (base64_screenshot, success, error_message)
        """
        logger.info(f'üì∏ Capturing screenshot for: {url}')
        
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=True,
                    args=['--no-sandbox', '--disable-setuid-sandbox']
                )
                
                page = await browser.new_page(
                    viewport={'width': 1920, 'height': 1080}
                )
                
                # Navigate with timeout
                await page.goto(url, {
                    'wait_until': 'networkidle',
                    'timeout': 30000
                })
                
                # Take screenshot
                screenshot = await page.screenshot(
                    full_page=True,
                    type='png'
                )
                
                logger.info('‚úÖ Screenshot captured')
                
                # Convert to base64 for OpenAI
                base64_screenshot = base64.b64encode(screenshot).decode('utf-8')
                screenshot_data_url = f'data:image/png;base64,{base64_screenshot}'
                
                await browser.close()
                
                return screenshot_data_url, True, None
                
        except Exception as error:
            logger.error(f'‚ùå Screenshot error: {error}')
            return None, False, f'–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å–∫—Ä–∏–Ω—à–æ—Ç: {str(error)}'
    
    async def analyze_screenshot_for_ads(self, url: str, screenshot_data_url: str) -> Tuple[Optional[Dict], bool, Optional[str]]:
        """
        Analyze screenshot using OpenAI Vision API to identify ad placement opportunities.
        
        Returns:
            Tuple of (analysis_result, success, error_message)
        """
        logger.info('ü§ñ Analyzing screenshot with OpenAI Vision...')
        
        if not self.openai_client:
            return None, False, 'OpenAI API key is not configured'
        
        try:
            # Remove data URL prefix for API call
            base64_image = screenshot_data_url.split(',')[1]
            
            response = self.openai_client.chat.completions.create(
                model='gpt-4o',  # Supports vision
                messages=[{
                    'role': 'user',
                    'content': [
                        {
                            'type': 'text',
                            'text': f'''–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–∫—Ä–∏–Ω—à–æ—Ç —Å–∞–π—Ç–∞ {url} –∏ –æ–ø—Ä–µ–¥–µ–ª–∏ —Ä–µ–∫–ª–∞–º–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏.

–í–∏–∑—É–∞–ª—å–Ω–æ –æ—Ü–µ–Ω–∏ –≥–¥–µ –º–æ–∂–Ω–æ —Ä–∞–∑–º–µ—Å—Ç–∏—Ç—å —Ä–µ–∫–ª–∞–º—É:
1. Header (—à–∞–ø–∫–∞ —Å–∞–π—Ç–∞, –Ω–∞–≤–∏–≥–∞—Ü–∏—è)
2. Sidebar (–±–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å —Å–ø—Ä–∞–≤–∞ –∏–ª–∏ —Å–ª–µ–≤–∞)  
3. Content (–≤–Ω—É—Ç—Ä–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, –º–µ–∂–¥—É –±–ª–æ–∫–∞–º–∏)
4. Footer (–ø–æ–¥–≤–∞–ª —Å–∞–π—Ç–∞)
5. Popup (–º–æ–¥–∞–ª—å–Ω—ã–µ –æ–∫–Ω–∞)

–î–ª—è –∫–∞–∂–¥–æ–π –∑–æ–Ω—ã —É–∫–∞–∂–∏:
- name: –Ω–∞–∑–≤–∞–Ω–∏–µ –∑–æ–Ω—ã
- available: true –µ—Å–ª–∏ –º–µ—Å—Ç–æ —Å–≤–æ–±–æ–¥–Ω–æ, false –µ—Å–ª–∏ —É–∂–µ –∑–∞–Ω—è—Ç–æ —Ä–µ–∫–ª–∞–º–æ–π
- size: —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —Ä–∞–∑–º–µ—Ä –±–∞–Ω–Ω–µ—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä "728x90", "300x250")
- priority: "high" –¥–ª—è —Å–∞–º—ã—Ö –∑–∞–º–µ—Ç–Ω—ã—Ö –º–µ—Å—Ç, "medium" –¥–ª—è –º–µ–Ω–µ–µ –∑–∞–º–µ—Ç–Ω—ã—Ö
- description: –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –≥–¥–µ –∏–º–µ–Ω–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –∑–æ–Ω–∞ –∏ –ø–æ—á–µ–º—É –æ–Ω–∞ –ø–æ–¥—Ö–æ–¥–∏—Ç

–í–ê–ñ–ù–û: –†–µ–∞–ª—å–Ω–æ –æ—Ü–µ–Ω–∏–≤–∞–π - –µ—Å—Ç—å –ª–∏ —Å–≤–æ–±–æ–¥–Ω–æ–µ –º–µ—Å—Ç–æ –∏–ª–∏ –≤—Å—ë —É–∂–µ –∑–∞–Ω—è—Ç–æ.

–í–µ—Ä–Ω–∏ JSON:
{{
  "zones": [
    {{
      "name": "Header",
      "available": true,
      "size": "728x90",
      "priority": "high",
      "description": "..."
    }}
  ],
  "language": "ru" or "en" (–æ–ø—Ä–µ–¥–µ–ª–∏ —è–∑—ã–∫ —Å–∞–π—Ç–∞)
}}'''
                        },
                        {
                            'type': 'image_url',
                            'image_url': {
                                'url': screenshot_data_url,
                                'detail': 'high'
                            }
                        }
                    ]
                }],
                response_format={'type': 'json_object'},
                max_tokens=2000
            )
            
            result = json.loads(response.choices[0].message.content)
            logger.info('‚úÖ Vision analysis complete')
            return result, True, None
            
        except Exception as error:
            logger.error(f'‚ùå Vision analysis error: {error}')
            return None, False, f'OpenAI Vision API error: {str(error)}'
    
    async def scrape_website_data(self, url: str) -> Dict:
        """
        Scrape website for emails and company information.
        
        Returns:
            Dict with emails, company_name, title, description
        """
        logger.info('üîç Scraping website data...')
        
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=True,
                    args=['--no-sandbox', '--disable-setuid-sandbox']
                )
                
                page = await browser.new_page()
                await page.goto(url, {
                    'wait_until': 'networkidle',
                    'timeout': 30000
                })
                
                html_content = await page.content()
                await browser.close()
                
                soup = BeautifulSoup(html_content, 'html.parser')
                
                # Extract emails
                emails = []
                email_regex = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'
                
                # Search in text content
                text = soup.get_text()
                found_emails = re.findall(email_regex, text)
                emails.extend(found_emails)
                
                # Search in mailto links
                for link in soup.find_all('a', href=lambda x: x and x.startswith('mailto:')):
                    email = link['href'].replace('mailto:', '')
                    emails.append(email)
                
                # Extract company name
                company_name = None
                
                # Try meta tags
                company_name = (
                    soup.find('meta', property='og:site_name') or
                    soup.find('meta', attrs={'name': 'author'})
                )
                if company_name:
                    company_name = company_name.get('content')
                else:
                    # Try title
                    title_tag = soup.find('title')
                    if title_tag:
                        company_name = title_tag.get_text().split('|')[0].strip()
                
                # Try footer for Russian company formats
                if not company_name:
                    footer_text = soup.find('footer')
                    if footer_text:
                        footer_content = footer_text.get_text()
                        match = re.search(r'(–û–û–û|–ò–ü|–ê–û|–ó–ê–û|–ü–ê–û)\s+["¬´]?([^"¬ª\n]+)["¬ª]?', footer_content)
                        if match:
                            company_name = match.group(0)
                
                # Clean and deduplicate emails
                unique_emails = list(set(email.strip() for email in emails if email and '@' in email))
                
                result = {
                    'emails': unique_emails,
                    'company_name': company_name,
                    'title': soup.find('title').get_text() if soup.find('title') else None,
                    'description': soup.find('meta', attrs={'name': 'description'}).get('content') if soup.find('meta', attrs={'name': 'description'}) else None
                }
                
                logger.info(f'‚úÖ Found {len(unique_emails)} emails, company: {company_name}')
                return result
                
        except Exception as error:
            logger.error(f'‚ùå Scraping error: {error}')
            return {'emails': [], 'company_name': None, 'title': None, 'description': None}
    
    async def research_company_owner(self, company_name: str, website_url: str) -> Dict:
        """
        Research company information using OpenAI.
        
        Returns:
            Dict with insights about the company
        """
        logger.info('üîé Researching company owner...')
        
        if not company_name:
            return {'insights': '–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞'}
        
        if not self.openai_client:
            return {'insights': 'OpenAI API key is not configured'}
        
        try:
            prompt = f'''–ù–∞–π–¥–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–º–ø–∞–Ω–∏–∏ "{company_name}" (—Å–∞–π—Ç: {website_url}).

–ò—Å–ø–æ–ª—å–∑—É—è –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –Ω–∞–π–¥–∏:
1. –ü–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∞—è —Ñ–æ—Ä–º–∞ (–û–û–û, –ò–ü –∏ —Ç.–¥.)
2. –ò–º—è —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è/–¥–∏—Ä–µ–∫—Ç–æ—Ä–∞ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
3. –û—Å–Ω–æ–≤–Ω–∞—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–æ–º–ø–∞–Ω–∏–∏
4. –ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Ñ–∞–∫—Ç—ã –∏–ª–∏ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è

–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç - —á–µ—Å—Ç–Ω–æ –Ω–∞–ø–∏—à–∏ —á—Ç–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.

–í–µ—Ä–Ω–∏ –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç—á—ë—Ç (3-5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π) –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.'''
            
            response = self.openai_client.chat.completions.create(
                model='gpt-4o-mini',
                messages=[{'role': 'user', 'content': prompt}],
                max_tokens=500
            )
            
            insights = response.choices[0].message.content
            logger.info('‚úÖ Research complete')
            return {'insights': insights}
            
        except Exception as error:
            logger.error(f'‚ùå Research error: {error}')
            return {'insights': f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏: {str(error)}'}
    
    async def generate_personalized_proposal(self, data: Dict) -> str:
        """
        Generate personalized commercial proposal.
        
        Args:
            data: Dict containing website_url, zones, language, company_name, owner_info, emails
            
        Returns:
            Generated proposal text
        """
        logger.info('‚úçÔ∏è Generating personalized proposal...')
        
        if not self.openai_client:
            return 'OpenAI API key is not configured. Please set OPENAI_API_KEY environment variable.'
        
        try:
            website_url = data['website_url']
            zones = data['zones']
            language = data['language']
            company_name = data.get('company_name')
            owner_info = data.get('owner_info', {})
            emails = data.get('emails', [])
            
            # Determine template language
            is_english = language == 'en'
            
            available_zones = [z for z in zones if z.get('available')]
            zones_description = '\n'.join([
                f"{i+1}. {zone['name']} ‚Äî {zone['description']}"
                for i, zone in enumerate(available_zones)
            ])
            
            if is_english:
                prompt = f'''Generate a personalized commercial proposal in ENGLISH for advertising placement.
                
Website: {website_url}
Company: {company_name or 'Website owner'}
Owner info: {owner_info.get('insights', 'Not available')}
Available ad zones: 
{zones_description}

Write a professional email following this structure:
1. Greeting (personalized if owner name available)
2. Compliment about their website/content
3. Brief about Adlook company
4. List of advertising opportunities
5. Call to action

Be professional and persuasive. Full email in English.'''
            else:
                prompt = f'''–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ –†–£–°–°–ö–û–ú —è–∑—ã–∫–µ.

–°–∞–π—Ç: {website_url}
–ö–æ–º–ø–∞–Ω–∏—è: {company_name or '–í–ª–∞–¥–µ–ª–µ—Ü —Å–∞–π—Ç–∞'}
–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–ª–∞–¥–µ–ª—å—Ü–µ: {owner_info.get('insights', '–ù–µ –Ω–∞–π–¥–µ–Ω–∞')}
–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–µ–∫–ª–∞–º–Ω—ã–µ –º–µ—Å—Ç–∞:
{zones_description}

–ù–∞–ø–∏—à–∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –ø–∏—Å—å–º–æ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ:
1. –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ (–ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –µ—Å–ª–∏ –µ—Å—Ç—å –∏–º—è)
2. –ö–æ–º–ø–ª–∏–º–µ–Ω—Ç –ø—Ä–æ –∏—Ö —Å–∞–π—Ç/–∫–æ–Ω—Ç–µ–Ω—Ç (–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏)
3. –ö—Ä–∞—Ç–∫–æ –ø—Ä–æ –∫–æ–º–ø–∞–Ω–∏—é Adlook
4. –°–ø–∏—Å–æ–∫ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π
5. –ü—Ä–∏–∑—ã–≤ –∫ –¥–µ–π—Å—Ç–≤–∏—é

–ò—Å–ø–æ–ª—å–∑—É–π —à–∞–±–ª–æ–Ω –∏–∑ –ø—Ä–∏–º–µ—Ä–∞ Adlook. –ë–µ–∑ –∑–≤—ë–∑–¥–æ—á–µ–∫ (*). –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ç–æ–Ω.'''
            
            response = self.openai_client.chat.completions.create(
                model='gpt-4o-mini',
                messages=[{'role': 'user', 'content': prompt}],
                max_tokens=1500,
                temperature=0.7
            )
            
            proposal = response.choices[0].message.content
            logger.info('‚úÖ Proposal generated')
            return proposal
            
        except Exception as error:
            logger.error(f'‚ùå Proposal generation error: {error}')
            return f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: {str(error)}'
    
    async def analyze_website_complete(self, url: str) -> Dict:
        """
        Complete workflow that orchestrates all analysis steps.
        
        Returns:
            Dict with all analysis results
        """
        logger.info('\nüöÄ === STARTING COMPLETE ANALYSIS ===\n')
        
        try:
            # Step 1: Capture screenshot
            logger.info('STEP 1: Screenshot')
            screenshot_data_url, screenshot_success, screenshot_error = await self.capture_screenshot(url)
            
            if not screenshot_success:
                return {
                    'success': False,
                    'error': f'Failed to capture screenshot: {screenshot_error}'
                }
            
            # Step 2: Vision analysis
            logger.info('\nSTEP 2: Vision Analysis')
            vision_result, vision_success, vision_error = await self.analyze_screenshot_for_ads(url, screenshot_data_url)
            
            if not vision_success:
                return {
                    'success': False,
                    'error': f'Failed to analyze screenshot: {vision_error}'
                }
            
            # Step 3: Scrape website data
            logger.info('\nSTEP 3: Scraping')
            scraped_data = await self.scrape_website_data(url)
            
            # Step 4: Research company
            logger.info('\nSTEP 4: Research')
            owner_info = await self.research_company_owner(scraped_data.get('company_name'), url)
            
            # Step 5: Generate proposal
            logger.info('\nSTEP 5: Generate Proposal')
            proposal = await self.generate_personalized_proposal({
                'website_url': url,
                'zones': vision_result.get('zones', []),
                'language': vision_result.get('language', 'en'),
                'company_name': scraped_data.get('company_name'),
                'owner_info': owner_info,
                'emails': scraped_data.get('emails', [])
            })
            
            logger.info('\n‚úÖ === ANALYSIS COMPLETE ===\n')
            
            return {
                'success': True,
                'screenshot': screenshot_data_url,
                'zones': vision_result.get('zones', []),
                'language': vision_result.get('language', 'en'),
                'emails': scraped_data.get('emails', []),
                'company_name': scraped_data.get('company_name'),
                'title': scraped_data.get('title'),
                'description': scraped_data.get('description'),
                'owner_info': owner_info.get('insights'),
                'proposal': proposal
            }
            
        except Exception as error:
            logger.error(f'\n‚ùå === ANALYSIS FAILED ===')
            logger.error(f'Error: {error}')
            return {
                'success': False,
                'error': str(error)
            }


# Global parser instance
parser = CompleteWebsiteParser()


async def analyze_website_complete(url: str) -> Dict:
    """
    Convenience function to analyze a website completely.
    
    Args:
        url: Website URL to analyze
        
    Returns:
        Dict with complete analysis results
    """
    return await parser.analyze_website_complete(url)